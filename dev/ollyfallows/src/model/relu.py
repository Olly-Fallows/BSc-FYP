# This is a layer that allows the relu activation function to be applied to the
# matrix that is currently going through the network

class relu(component):

    def __init__(self):
        pass
